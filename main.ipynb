{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Failure Prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import kaggle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, plot_confusion_matrix\n",
    "\n",
    "KAGGLE_DATASET = \"andrewmvd/heart-failure-clinical-data\"\n",
    "FILE_NAME = \"heart_failure_clinical_records_dataset.csv\"\n",
    "cwd = os.getcwd()\n",
    "DATASET_FOLDER_PATH = os.path.join(cwd, \"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataset():\n",
    "    \"\"\"\n",
    "    Downloads the dataset from Kaggle if not present\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(FILE_NAME):\n",
    "        os.makedirs(DATASET_FOLDER_PATH, exist_ok=True)\n",
    "        kaggle.api.authenticate()\n",
    "        kaggle.api.dataset_download_files(KAGGLE_DATASET, path=DATASET_FOLDER_PATH, unzip=True)\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Returns the dataframe containing the dataset\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(DATASET_FOLDER_PATH, FILE_NAME)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "fetch_dataset()\n",
    "dataset = load_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "Description of the features:\n",
    "- **age**: Age of the patient\n",
    "- **anaemia**: Decrease of red blood cells or hemoglobin (boolean)\n",
    "- **creatinine_phosphokinase**: Level of the CPK enzyme in the blood (mcg/L)\n",
    "- **diabetes**: If the patient has diabetes (boolean)\n",
    "- **ejection_fraction**: Percentage of blood leaving the heart at each contraction (percentage)\n",
    "- **high_blood_pressure**: If the patient has hypertension (boolean)\n",
    "- **platelets**: Platelets in the blood (kiloplatelets/mL)\n",
    "- **serum_creatinine**: Level of serum creatinine in the blood (mg/dL)\n",
    "- **serum_sodium**: Level of serum sodium in the blood (mEq/L)\n",
    "- **sex**: Woman or man (binary)\n",
    "- **smoking**: If the patient smokes (binary)\n",
    "- **time**: Nan\n",
    "- **DEATH_EVENT**: If the heart attack leads to death (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking for Null values:\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,30))\n",
    "\n",
    "for i, column in enumerate(dataset.drop([\"DEATH_EVENT\"], axis=1).columns, 1):\n",
    "    plt.subplot(4,3,i)\n",
    "    sns.histplot(dataset[column])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Correlation of the different features:\n",
    "As we can see **age, serum_creatinine** are linearly correlated with DEATH_EVENT; while **time, ejection_fraction, serum_sodium** are inversely correlated with DEATH_EVENT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dataset.corr()\n",
    "ax, fig = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(corr, vmax=1,vmin=-1, annot=True, linewidths=.5, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a custom Transformer to select only the highly correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correlated features with >0.1 correlation\n",
    "corr_f = corr[abs(corr['DEATH_EVENT']) > 0.1]['DEATH_EVENT']\n",
    "print(corr_f)\n",
    "# Get the index(column names) of them\n",
    "corr_i = list(corr_f.index)\n",
    "print(corr_i)\n",
    "# get thier colum index\n",
    "#print([dataset.columns.get_loc(c) for c in corr_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearlyCorrelatedFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that transforms X to have only the features that are linearly correlated to y. The threshold \n",
    "    for the amount of correlation can be controlled.\n",
    "\n",
    "    Requires df that have labeled columns.\n",
    "    Pipeline will run this: self.fit(X, y, **fit_params).transform(X); Thus need X,y for fit() and it should return\n",
    "    \"self\" thus making it possible it chaing like that, ie self.fit.transform. Need only X for transform()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, correlation_threshold=0.1):\n",
    "        import pandas as pd\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.final_col = []\n",
    "\n",
    "    def merge_X_y(self,X,y):\n",
    "        \"\"\"\n",
    "        Merges X and y and preserves the labels.\n",
    "        \"\"\"\n",
    "        X_col = list(X.columns)\n",
    "        y_col = list(y.columns)\n",
    "        df_col = X_col + y_col \n",
    "        df = pd.concat([X,y], axis=1)\n",
    "        df.columns = df_col\n",
    "\n",
    "        return df        \n",
    "\n",
    "    def get_correlated_features(self, X, y):\n",
    "        \"\"\"\n",
    "        Return a list of column labels that are correlated to \"y\". Includes label of \"y\" aswell\n",
    "        args:\n",
    "            X: mxn numpy array\n",
    "            y: mx1 numpy vector\n",
    "        \"\"\"\n",
    "        dataset = self.merge_X_y(X,y)\n",
    "        y_col = str(list(dataset.columns)[-1])\n",
    "\n",
    "        corr = dataset.corr()\n",
    "        # Get the correlated features with correlation>threhsold\n",
    "        #y_label = y_col[0]\n",
    "        corr_top = corr[abs(corr[y_col]) > self.correlation_threshold][y_col]\n",
    "        # list of index(column names in dataset)\n",
    "        corr_features = list(corr_top.index)\n",
    "\n",
    "        return corr_features\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Selects the correlated features and saves it in \"final_col\"\n",
    "        \"\"\"\n",
    "        if isinstance(X,pandas.core.frame.DataFrame) and isinstance(y, pandas.core.series.Series):\n",
    "            print(\"GG\")\n",
    "        X = X.values\n",
    "        y = y.values\n",
    "        self.final_col = self.get_correlated_features(X,y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Returns a new X with only the selected features.\n",
    "        \"\"\"\n",
    "\n",
    "        return X.filter(self.final_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating X,y\n",
    "output_col = \"DEATH_EVENT\"\n",
    "X = dataset.drop([output_col], axis=1)\n",
    "y = dataset[output_col]\n",
    "y.columns=[output_col]\n",
    "\n",
    "# Include lin corr features alone\n",
    "#lin_corr = LinearlyCorrelatedFeatures(correlation_threshold=0.1)\n",
    "#X = lin_corr.transform(X, y)\n",
    "\n",
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42, test_size=0.2)\n",
    "\n",
    "# Standardize the training set using the training set means and standard deviations. \n",
    "# Standardize Test set using the training set means and standard deviations.\n",
    "#std_scalar = StandardScaler()\n",
    "#std_scalar.fit(X_train)\n",
    "#print(std_scalar.mean_)\n",
    "#X_train = std_scalar.transform(X_train)\n",
    "#X_test = std_scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"corr_features\", LinearlyCorrelatedFeatures(correlation_threshold=0.1)),\n",
    "    (\"std_scalar\", StandardScaler()),\n",
    "])\n",
    "\n",
    "X_train_p = pipeline.fit_transform(X_train,y_train)\n",
    "X_test_p = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid_scores(grid_search):\n",
    "    \"\"\"\n",
    "    Prints the Mean scores and its corresponding hyperparameter for each candidate in GridSearchCV\n",
    "    \"\"\"\n",
    "    grid_scores = grid_search.cv_results_\n",
    "    best_mean_score = 0\n",
    "    best_param = None\n",
    "    for mean_score, params in zip(grid_scores[\"mean_test_score\"], grid_scores[\"params\"] ):\n",
    "        if mean_score > best_mean_score:\n",
    "            best_mean_score = mean_score\n",
    "            best_param = params\n",
    "\n",
    "        print(\"%.4f\" % mean_score, params)\n",
    "    print(f\"Best Param:\", \"%.4f\" % best_mean_score, best_param)\n",
    "\n",
    "def print_test_scores(model, X_test=X_test, y_test=y_test):\n",
    "    \"\"\"\n",
    "    Use the given model to test the model on the Test Set\n",
    "    Then display the various scores and confusion matrix by comparing against y_test\n",
    "    \"\"\"\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evalution\n",
    "    acc = accuracy_score(y_test, y_test_pred)*100\n",
    "    pre = precision_score(y_test, y_test_pred)*100\n",
    "    rec = recall_score(y_test, y_test_pred)*100\n",
    "    f1 = f1_score(y_test, y_test_pred)*100\n",
    "    print('Accuracy Score : ', \"{:.2f}%\".format(acc))\n",
    "    print('Precision Score : ', \"{:.2f}%\".format(pre))\n",
    "    print('Recall Score : ', \"{:.2f}%\".format(rec))\n",
    "    print('F1 Score : ', \"{:.2f}%\".format(f1))\n",
    "    plot_confusion_matrix(model, X_test, y_test)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "REFIT = True\n",
    "SCORING = \"f1\"\n",
    "CV = 5\n",
    "N_JOBS = 10\n",
    "\n",
    "log_reg = LogisticRegression(random_state=RANDOM_STATE)\n",
    "log_reg_parameters = {\n",
    "    \"penalty\": [\"l2\"],\n",
    "    'C': [0.1, 0.5 ,1]\n",
    "    }\n",
    "log_reg_grid = GridSearchCV(log_reg, log_reg_parameters, refit=REFIT, scoring = SCORING, verbose=2, cv=CV, n_jobs=N_JOBS)\n",
    "log_reg_grid.fit(X_train, y_train)\n",
    "print_grid_scores(log_reg_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since refit=True, the model is refitted using the best parameter and is available via best_estimator\n",
    "log_reg_best = log_reg_grid.best_estimator_\n",
    "print_test_scores(log_reg_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state=RANDOM_STATE)\n",
    "svc_parameters = {\n",
    "    'C': [0.1, 0.5, 1, 5, 10],\n",
    "    'kernel': ['linear'],\n",
    "}\n",
    "svc_grid = GridSearchCV(svc, svc_parameters, refit=REFIT, scoring = SCORING, verbose=2, cv=CV, n_jobs=N_JOBS)\n",
    "svc_grid.fit(X_train, y_train)\n",
    "print_grid_scores(svc_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best = svc_grid.best_estimator_\n",
    "print_test_scores(svc_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "dt_parameters = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [1,None],\n",
    "    'max_features': [2,None],\n",
    "    'max_leaf_nodes': [1,2,None]\n",
    "}\n",
    "dt_grid = GridSearchCV(dt, dt_parameters, refit=REFIT, scoring = SCORING, verbose=2, cv=CV, n_jobs=N_JOBS)\n",
    "dt_grid.fit(X_train, y_train)\n",
    "print_grid_scores(dt_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best = dt_grid.best_estimator_\n",
    "print_test_scores(dt_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "rf_parameters = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [2,5,None],\n",
    "    'n_estimators': [100,None],\n",
    "    'max_leaf_nodes': [5,8, None]\n",
    "}\n",
    "rf_grid = GridSearchCV(rf, rf_parameters, refit=REFIT, scoring = SCORING, verbose=2, cv=CV, n_jobs=N_JOBS)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print_grid_scores(rf_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = rf_grid.best_estimator_\n",
    "print_test_scores(rf_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "gb_clf_parameters = {\n",
    "    'loss': ['deviance', 'exponential'],\n",
    "    'learning_rate': [0.1, 0.5, 0.05],\n",
    "    'n_estimators': [50,100,150],\n",
    "    'max_depth': [1,3,5],\n",
    "\n",
    "}\n",
    "gb_clf_grid = GridSearchCV(gb_clf, gb_clf_parameters, refit=REFIT, scoring = SCORING, verbose=2, cv=CV, n_jobs=N_JOBS)\n",
    "gb_clf_grid.fit(X_train, y_train)\n",
    "print_grid_scores(gb_clf_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_scores(gb_clf_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf_parameters = {\n",
    "    'n_neighbors': [2,3,5,8,10],\n",
    "}\n",
    "knn_clf_grid = GridSearchCV(knn_clf, knn_clf_parameters, refit=REFIT, scoring = SCORING, verbose=2, cv=CV, n_jobs=N_JOBS)\n",
    "knn_clf_grid.fit(X_train, y_train)\n",
    "print_grid_scores(knn_clf_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_scores(knn_clf_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression:\n",
    "Accuracy Score :  78.33%\n",
    "\n",
    "Precision Score :  92.86%\n",
    "\n",
    "Recall Score :  52.00%\n",
    "\n",
    "F1 Score :  66.67%\n",
    "\n",
    "## 2. Support Vector Classifier:\n",
    "Accuracy Score :  78.33%\n",
    "\n",
    "Precision Score :  87.50%\n",
    "\n",
    "Recall Score :  56.00%\n",
    "\n",
    "F1 Score :  68.29%\n",
    "\n",
    "## 3. Decision Tree Classifier:\n",
    "Accuracy Score :  75.00%\n",
    "\n",
    "Precision Score :  81.25%\n",
    "\n",
    "Recall Score :  52.00%\n",
    "\n",
    "F1 Score :  63.41%\n",
    "\n",
    "## 4. Random Forst Classifier:\n",
    "Accuracy Score :  75.00%\n",
    "\n",
    "Precision Score :  81.25%\n",
    "\n",
    "Recall Score :  52.00%\n",
    "\n",
    "F1 Score :  63.41%\n",
    "\n",
    "## 5. Gradient Boosting Classifier:\n",
    "Accuracy Score :  76.67%\n",
    "\n",
    "Precision Score :  92.31%\n",
    "\n",
    "Recall Score :  48.00%\n",
    "\n",
    "F1 Score :  63.16%\n",
    "\n",
    "## 6. K Nearest Neighbors:\n",
    "Accuracy Score :  71.67%\n",
    "\n",
    "Precision Score :  75.00%\n",
    "\n",
    "Recall Score :  48.00%\n",
    "\n",
    "F1 Score :  58.54%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}